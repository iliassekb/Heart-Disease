{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9982d21",
   "metadata": {},
   "source": [
    "\n",
    "# ECG Binary Classification (PyTorch) â€” From Normalization to Evaluation\n",
    "\n",
    "This notebook trains a **binary CNN** on an ECG image dataset structured as:\n",
    "\n",
    "```\n",
    "processed_dataset/\n",
    "  Anormal/\n",
    "  Normal/\n",
    "```\n",
    "\n",
    "It covers:\n",
    "1. Mean/Std computation (for normalization).\n",
    "2. Dataset split (80/10/10) with stratification.\n",
    "3. PyTorch DataLoaders (GPU-ready).\n",
    "4. Simple CNN (binary head with BCEWithLogitsLoss).\n",
    "5. Training loop with AMP (mixed precision), tracking **loss** and **accuracy**.\n",
    "6. Plots for loss & accuracy.\n",
    "7. Final evaluation (Accuracy, Precision, Recall, F1, AUC, Confusion Matrix, Classification Report).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 0) Setup & Config =====\n",
    "import os, numpy as np, torch, matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED     = 42\n",
    "IMG_SIZE = 224\n",
    "BATCH    = 64\n",
    "ROOT     = \"processed_dataset\"   \n",
    "# Reproducibility & GPU\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.backends.cudnn.benchmark = True  # speed up CNN on fixed input sizes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4908827",
   "metadata": {},
   "source": [
    "## 1) Compute mean & std (grayscale, in [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d67e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Temporary transform to compute mean/std (no Normalize yet)\n",
    "tx_tmp = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),           # -> [0,1]\n",
    "])\n",
    "\n",
    "ds_tmp = datasets.ImageFolder(ROOT, transform=tx_tmp)\n",
    "loader_tmp = DataLoader(ds_tmp, batch_size=128, num_workers=min(8, os.cpu_count() or 1), shuffle=False)\n",
    "\n",
    "psum = 0.0\n",
    "psum_sq = 0.0\n",
    "count = 0\n",
    "\n",
    "for x, _ in loader_tmp:\n",
    "    b = x.size(0)        # [B,1,H,W]\n",
    "    x = x.view(b, -1)\n",
    "    psum    += x.sum()\n",
    "    psum_sq += (x**2).sum()\n",
    "    count   += x.numel()\n",
    "\n",
    "mean = (psum / count).item()\n",
    "std  = ((psum_sq / count - mean**2) ** 0.5).item()\n",
    "print(f\"mean={mean:.6f}, std={std:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c42ff",
   "metadata": {},
   "source": [
    "## 2) Dataset with Normalize & Train/Val/Test split (80/10/10, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d216d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MEAN, STD = mean, std\n",
    "\n",
    "tx = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),                 # -> [0,1]\n",
    "    transforms.Normalize([MEAN], [STD]),   # normalization\n",
    "])\n",
    "\n",
    "ds = datasets.ImageFolder(ROOT, transform=tx)\n",
    "y  = np.array(ds.targets)\n",
    "idx = np.arange(len(ds))\n",
    "\n",
    "# 80/20 -> then 10/10 from the 20\n",
    "idx_tr, idx_tmp, y_tr, y_tmp = train_test_split(\n",
    "    idx, y, test_size=0.20, random_state=SEED, stratify=y\n",
    ")\n",
    "idx_va, idx_te, y_va, y_te = train_test_split(\n",
    "    idx_tmp, y_tmp, test_size=0.50, random_state=SEED, stratify=y_tmp\n",
    ")\n",
    "\n",
    "train_set, val_set, test_set = Subset(ds, idx_tr), Subset(ds, idx_va), Subset(ds, idx_te)\n",
    "\n",
    "num_workers = min(8, os.cpu_count() or 1)\n",
    "pin = (device.type == \"cuda\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=pin, persistent_workers=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=BATCH, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=pin, persistent_workers=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=BATCH, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=pin, persistent_workers=True)\n",
    "\n",
    "print(\"Classes:\", ds.classes, ds.class_to_idx)  # e.g. ['Anormal','Normal'] -> {'Anormal':0,'Normal':1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c33f6",
   "metadata": {},
   "source": [
    "## 3) Model (binary head with BCEWithLogitsLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498900c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleECGCNN(nn.Module):\n",
    "    def __init__(self, img_size=IMG_SIZE):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),   # 112x112\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # 56x56\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # 28x28\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * (img_size//8) * (img_size//8), 1)              # single logit\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleECGCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()                 # stable (Sigmoid + BCE in one)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# optional: learning-rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076122e",
   "metadata": {},
   "source": [
    "## 4) Training with AMP (loss & accuracy tracked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a06c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "def epoch_pass(model, loader, device, optimizer=None, criterion=None, threshold=0.5):\n",
    "    train_mode = optimizer is not None\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_samples  = 0\n",
    "    n_correct  = 0\n",
    "\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.float().unsqueeze(1).to(device, non_blocking=True)  # [B,1]\n",
    "        bs = xb.size(0)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model(xb)                 # [B,1]\n",
    "                loss   = criterion(logits, yb)     # BCEWithLogitsLoss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            with torch.no_grad(), autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                logits = model(xb)\n",
    "                loss   = criterion(logits, yb)\n",
    "\n",
    "        # Accuracy\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).long()\n",
    "        y_int = yb.long()\n",
    "\n",
    "        n_correct  += (preds == y_int).sum().item()\n",
    "        n_samples  += bs\n",
    "        total_loss += loss.item() * bs\n",
    "\n",
    "    mean_loss = total_loss / max(1, n_samples)\n",
    "    acc = n_correct / max(1, n_samples)\n",
    "    return mean_loss, acc\n",
    "\n",
    "EPOCHS = 15\n",
    "best_val = float('inf')\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = epoch_pass(model, train_loader, device, optimizer=optimizer, criterion=criterion)\n",
    "    va_loss, va_acc = epoch_pass(model, val_loader,   device, optimizer=None,  criterion=criterion)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    scheduler.step(va_loss)\n",
    "\n",
    "    print(f\"[{epoch:02d}/{EPOCHS}] \"\n",
    "          f\"train_loss={tr_loss:.4f} train_acc={tr_acc*100:.2f}% \"\n",
    "          f\"val_loss={va_loss:.4f}   val_acc={va_acc*100:.2f}%\" )\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        best_state = {\"epoch\": epoch,\n",
    "                      \"model\": model.state_dict(),\n",
    "                      \"optimizer\": optimizer.state_dict()}\n",
    "\n",
    "# Save best model checkpoint\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "torch.save(best_state, \"checkpoints/best_cnn_binary.pt\")\n",
    "print(\"\\nBest epoch:\", best_state[\"epoch\"], \"| val_loss:\", best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9cd0a",
   "metadata": {},
   "source": [
    "## 5) Plot training/validation curves (Loss & Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd31029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "epochs = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "# Loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"train loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"],   label=\"val loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training / Validation Loss\"); plt.legend(); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_curve.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"train acc\")\n",
    "plt.plot(epochs, history[\"val_acc\"],   label=\"val acc\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Training / Validation Accuracy\"); plt.legend(); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acc_curve.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved figures: loss_curve.png, acc_curve.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb63594",
   "metadata": {},
   "source": [
    "## 6) Evaluation block (Accuracy, Precision, Recall, F1, AUC, Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a60d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_binary(model, loader, device, threshold=0.5, positive_label=1):\n",
    "    model.eval()\n",
    "    all_probs, all_targets = [], []\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        with torch.autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits = model(xb)            # [B,1]\n",
    "        probs = torch.sigmoid(logits).squeeze(1).float().cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(yb.long().cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "    y_prob = np.concatenate(all_probs, axis=0)\n",
    "    y_pred = (y_prob >= threshold).astype(np.int64)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", pos_label=positive_label\n",
    "    )\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec,\n",
    "            \"f1\": f1, \"auc\": auc, \"confusion_matrix\": cm, \"report\": report}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b62bdb",
   "metadata": {},
   "source": [
    "### Run final evaluation on **test** set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dfaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load best weights (optional, we already kept them in memory)\n",
    "best_ckpt = torch.load(\"checkpoints/best_cnn_binary.pt\", map_location=device)\n",
    "model.load_state_dict(best_ckpt[\"model\"])\n",
    "\n",
    "metrics = evaluate_binary(model, test_loader, device, threshold=0.5, positive_label=1)\n",
    "print(\"\\n=== Test metrics ===\")\n",
    "print(f\"Accuracy : {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall   : {metrics['recall']:.4f}\")\n",
    "print(f\"F1       : {metrics['f1']:.4f}\")\n",
    "print(f\"AUC      : {metrics['auc']:.4f}\")\n",
    "print(\"Confusion matrix:\\n\", metrics[\"confusion_matrix\"])\n",
    "print(metrics[\"report\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
